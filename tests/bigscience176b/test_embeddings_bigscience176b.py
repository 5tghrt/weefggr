import json
import os
import torch
import unittest

import numpy as np

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel

class BigScienceEmbeddingTest(unittest.TestCase):
    """
        The goal here is to compare the embeddings generated by the model trained 
        using Megatron-LM with the one from the transformers library.
        You need to install tokenizers following this readme:
            - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles
        Main challenge:
            - Make this script scalable and usable on the big checkpoints

        Tokenizer used during training:
            - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles
    
        # TODO change the script (or just add skip) when building the env with tokenizers 0.12.0
    """

    def setUp(self):
        super().setUp()
        self.path_tokenizer = "bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles"
        self.tokenizer = AutoTokenizer.from_pretrained(self.path_tokenizer)
        self.path_bigscience_model = "/home/younes/Desktop/Work/data/megatron-debug/"

    def test_load_transformers_model(self):
        # TODO load the model (.bin file)
        try:
            _ = AutoModel.from_pretrained(self.path_bigscience_model)
        except:
            self.fail("Failed loading the model")
        # pass

    @torch.no_grad()
    def test_embeddings(self):
        model = AutoModel.from_pretrained(self.path_bigscience_model)
        model.eval()
        # TODO load the models (.bin file + transformers model) and compare the embeddings
        EMBEDDINGS_DS_BEFORE_LN_BF_16 = { 3478: 0.0002307891845703125, 368: -0.000568389892578125, 109586: -0.0003910064697265625, 35433: -0.000194549560546875, 2: 0.0004138946533203125, 77: 0.000659942626953125, 132619: -0.00031280517578125, 2175: 0.000457763671875, 23714: 0.000263214111328125, 73173: -0.000286102294921875, 144252: 0.00052642822265625}
        EMBEDDINGS_DS_BEFORE_LN_F_16 = {132619: -0.00031256675720214844, 3478: 0.00023090839385986328, 368: -0.0005702972412109375, 109586: -0.00039124488830566406, 35433: -0.000194549560546875, 2: 0.0004146099090576172, 2175: 0.0004572868347167969, 23714: 0.00026416778564453125, 73173: -0.0002865791320800781, 144252: 0.0005254745483398438, 77: 0.0006618499755859375}
        EMBEDDINGS_DS_BEFORE_LN_F_32 = {132619: -0.00031267106533050537, 3478: 0.00023087859153747559, 368: -0.0005701072514057159, 109586: -0.0003911703824996948, 35433: -0.0001944899559020996, 2: 0.0004146844148635864, 2175: 0.00045740045607089996, 23714: 0.0002641640603542328, 73173: -0.0002864748239517212, 144252: 0.0005256589502096176, 77: 0.0006617321632802486}

        TEST_EMBEDDINGS = {
            "torch.bfloat16":EMBEDDINGS_DS_BEFORE_LN_BF_16,
            "torch.float16":EMBEDDINGS_DS_BEFORE_LN_F_16,
            "torch.float":EMBEDDINGS_DS_BEFORE_LN_F_32,
            "torch.float32":EMBEDDINGS_DS_BEFORE_LN_F_32,
        }

        EXAMPLE_IDS = [  3478,    368, 109586,  35433,      2,     77, 132619,   3478,    368,
         109586,  35433,      2,   2175,  23714,  73173, 144252,      2,     77,
         132619,   3478]
        

        EMBEDDINGS_DS_AFTER_LN = {3478: -6.580352783203125e-05, 368: 0.0001316070556640625, 109586: -0.00030517578125, 35433: 4.00543212890625e-05, 2: -7.2479248046875e-05, 77: -8.96453857421875e-05, 132619: 0.0001583099365234375, 2175: 2.1219253540039062e-05, 23714: -0.000247955322265625, 73173: -0.00021839141845703125, 144252: -0.0001430511474609375}
        
        tensor_ids = torch.LongTensor([EXAMPLE_IDS])
        # position_ids = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])

        embeddings = model.word_embeddings(tensor_ids)
        # first check the embeddings before LN
        output_dict = {}
        for i, idx in enumerate(EXAMPLE_IDS):
            output_dict[idx] = embeddings.mean(dim=-1)[0][i].item()
        
        self.assertDictEqual(TEST_EMBEDDINGS[str(model.dtype)], output_dict)

        embeddings_ln = model.word_embeddings_layernorm(embeddings)
        mean_output_embeddings = embeddings_ln.mean(dim=-1)

        output_dict_norm = {}
        for i, idx in enumerate(EXAMPLE_IDS):
            output_dict_norm[idx] = mean_output_embeddings[0][i].item()
        # word_embeddings
        
        self.assertDictEqual(EMBEDDINGS_DS_AFTER_LN, output_dict_norm)
if __name__ == '__main__':
    unittest.main()