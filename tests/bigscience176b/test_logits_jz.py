import unittest

import torch

from transformers import AutoModel, AutoTokenizer
from transformers.models.bigscience176b import BigScience176BLMHeadModel


class BigScienceEmbeddingTest(unittest.TestCase):
    """
    The goal here is to compare the embeddings generated by the model trained
    using Megatron-LM with the one from the transformers library, with a small GPT2-like model
    to ensure that the conversion from Megatron-LM to transformers has been done successfully.
    The script compares the logits of the embedding layer and the transformer layers.

    WARNING: It is expected that these logits will not have exactly the same statistics when running
    the code on CPU or GPU. For more info, please visit:
      - https://github.com/pytorch/pytorch/issues/76052#issuecomment-1103193548
      - https://discuss.pytorch.org/t/reproducibility-issue-between-intel-and-amd-cpus/144779/9


    You need to install tokenizers following this readme:
        - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles

    Tokenizer used during training:
        - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles

    # TODO change the script (or just add skip) when building the env with tokenizers 0.12.0
    """

    def setUp(self):
        super().setUp()
        self.path_tokenizer = "bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles"
        self.tokenizer = AutoTokenizer.from_pretrained(self.path_tokenizer)
        self.path_bigscience_model = "/gpfswork/rech/six/uan68tv/model-conversion/tr11e-350M-transformers"

    @torch.no_grad()
    # @unittest.skip("demonstrating skipping")
    def test_logits(self):
        # TODO ifelse device
        model = AutoModel.from_pretrained(self.path_bigscience_model, use_cache=False)
        device_map = {
            0: [0, 1, 2],
            1: [3, 4, 5],
            2: [6, 7, 8],
            3: [9, 10, 11],
        }
        model.parallelize(device_map)
        model.eval()

        EXAMPLE_IDS = [
            3478,
            368,
            109586,
            35433,
            2,
            77,
            132619,
            3478,
            368,
            109586,
            35433,
            2,
            2175,
            23714,
            73173,
            144252,
            2,
            77,
            132619,
            3478,
        ]

        a = torch.randn(1, 1, 20, 20)
        ATTN_MASK = (torch.triu(a, diagonal=1) != 0).to("cuda:0")

        logits = model(EXAMPLE_IDS, attention_mask=ATTN_MASK).logits

        print("Logits: ", logits.mean().item())


if __name__ == "__main__":
    unittest.main()
