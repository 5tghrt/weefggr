import unittest

import torch
# import deepspeed

from transformers import AutoModel, AutoTokenizer
from transformers.models.bigscience176b import BigScience176BLMHeadModel
from transformers.models.bigscience176b.logits_utils import save_logits


class BigScienceEmbeddingTest(unittest.TestCase):
    """
    The goal here is to compare the embeddings generated by the model trained
    using Megatron-LM with the one from the transformers library, with a small GPT2-like model
    to ensure that the conversion from Megatron-LM to transformers has been done successfully.
    The script compares the logits of the embedding layer and the transformer layers.

    WARNING: It is expected that these logits will not have exactly the same statistics when running
    the code on CPU or GPU. For more info, please visit:
      - https://github.com/pytorch/pytorch/issues/76052#issuecomment-1103193548
      - https://discuss.pytorch.org/t/reproducibility-issue-between-intel-and-amd-cpus/144779/9


    You need to install tokenizers following this readme:
        - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles

    Tokenizer used during training:
        - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles

    # TODO change the script (or just add skip) when building the env with tokenizers 0.12.0
    """

    def setUp(self):
        super().setUp()
        # self.path_tokenizer = "bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles"
        # self.tokenizer = AutoTokenizer.from_pretrained(self.path_tokenizer)
        self.path_bigscience_model = "/gpfswork/rech/six/uan68tv/model-conversion/tr11e-350M-transformers"

    # @unittest.skip("demonstrating skipping")
    @torch.no_grad()
    def test_logits(self):
        # TODO ifelse device
        model = BigScience176BLMHeadModel.from_pretrained(self.path_bigscience_model, use_cache=False)
        # model = model.cuda()
        device_map = {0:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]}
        model.parallelize(device_map)
        model.eval()

        EXAMPLE_IDS = [[2175,  23714,  73173, 144252, 2, 77, 132619, 3478, 368, 109586, 35433, 2, 2175,  23714,  73173, 144252, 2, 2175, 23714, 73173]]

        # a = torch.randn(1, 1, 20, 20)
        # ATTN_MASK = (torch.triu(a, diagonal=1) != 0).to("cuda:0")
        ATTN_MASK = torch.triu(torch.ones(1, 1, 20, 20), diagonal=1).to("cuda:0").to(model.dtype)

        input_tensor = torch.LongTensor(EXAMPLE_IDS).to("cuda:0")

        logits = model(input_tensor, attention_mask=ATTN_MASK).logits

        print("Logits shape: ", logits.shape)
        print("Logits: ", logits.mean().item())
        print("Max: ", logits.max().item())
        print("Min: ", logits.min().item())
        print("Mean: ", logits.mean(dim=-1))
        print("Some values: ", logits[0,:, 0])
        print("Argmax: ", torch.argmax(logits, dim=-1))
        torch.save(logits, "/gpfswork/rech/six/uan68tv/data/tensors_to_test/logits_1_tr_apex_cu115.pt")
        save_logits('final_logit', logits, "after_final_emb", "transformers")
        EXAMPLE_IDS = [[144252, 2, 2175,  23714,  73173, 144252, 2, 77, 132619, 3478, 368, 109586,  35433, 2, 77, 132619,   3478,    368, 109586,  35433]]

        input_tensor = torch.LongTensor(EXAMPLE_IDS).to("cuda:0")

        logits = model(input_tensor, attention_mask=ATTN_MASK).logits
        torch.save(logits, "/gpfswork/rech/six/uan68tv/data/tensors_to_test/logits_2_tr_apex_cu115.pt")
        print("Logits2 shape: ", logits.shape)
        print("Logits2: ", logits.mean().item())
        print("Max: ", logits.max().item())
        print("Min: ", logits.min().item())
        print("Mean: ", logits.mean(dim=-1))
        print("Some values: ", logits[0,:, 0])
        print("Argmax: ", torch.argmax(logits, dim=-1))
        save_logits('final_logit', logits, "after_final_emb", "transformers")



if __name__ == "__main__":
    unittest.main()
